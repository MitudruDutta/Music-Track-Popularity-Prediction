{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Regression Model\n",
    "\n",
    "This notebook implements an ensemble regression model using XGBoost, Gradient Boosting, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Loading datasets...\")\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to preprocess date features\n",
    "def process_date_features(df):\n",
    "    # Convert to datetime\n",
    "    df['publication_timestamp'] = pd.to_datetime(df['publication_timestamp'], format='%d-%m-%Y', errors='coerce')\n",
    "    \n",
    "    # Extract features\n",
    "    df['release_year'] = df['publication_timestamp'].dt.year\n",
    "    df['release_month'] = df['publication_timestamp'].dt.month\n",
    "    df['release_day'] = df['publication_timestamp'].dt.day\n",
    "    df['days_since_2000'] = (df['publication_timestamp'] - pd.Timestamp('2000-01-01')).dt.days\n",
    "    \n",
    "    # Create cyclical features for month\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['release_month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['release_month']/12)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create interaction features\n",
    "def create_interaction_features(df):\n",
    "    # Create meaningful feature interactions\n",
    "    df['energy_dance_product'] = df['intensity_index_0'] * df['rhythmic_cohesion_0']\n",
    "    df['energy_dance_product_1'] = df['intensity_index_1'].fillna(0) * df['rhythmic_cohesion_1'].fillna(0)\n",
    "    df['energy_dance_product_2'] = df['intensity_index_2'].fillna(0) * df['rhythmic_cohesion_2'].fillna(0)\n",
    "    \n",
    "    # Song length features\n",
    "    df['avg_duration'] = (df['duration_ms_0'] + df['duration_ms_1'].fillna(0) + df['duration_ms_2'].fillna(0)) / 3\n",
    "    df['total_duration'] = df['duration_ms_0'] + df['duration_ms_1'].fillna(0) + df['duration_ms_2'].fillna(0)\n",
    "    \n",
    "    # Ratio features\n",
    "    df['organic_to_energy_0'] = df['organic_texture_0'] / (df['intensity_index_0'] + 0.001)\n",
    "    \n",
    "    # Feature aggregation across tracks\n",
    "    for metric in ['rhythmic_cohesion', 'intensity_index', 'organic_texture', 'emotional_resonance']:\n",
    "        for agg_func, suffix in [('mean', 'mean'), ('std', 'std'), ('max', 'max'), ('min', 'min')]:\n",
    "            cols = [f\"{metric}_{i}\" for i in range(3)]\n",
    "            df[f\"{metric}_{suffix}\"] = df[cols].apply(lambda x: getattr(np, agg_func)(x.dropna()), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process both datasets\n",
    "print(\"Processing features...\")\n",
    "for df in [train_df, test_df]:\n",
    "    df = process_date_features(df)\n",
    "    df = create_interaction_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify column types\n",
    "categorical_cols = ['weekday_of_release', 'season_of_release', 'lunar_phase']\n",
    "composition_cols = [col for col in train_df.columns if 'composition_label' in col or 'creator_collective' in col]\n",
    "categorical_cols.extend(composition_cols)\n",
    "\n",
    "# Check data types in train_df to ensure all object/string columns are treated as categorical\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype == 'object' or pd.api.types.is_string_dtype(train_df[col]):\n",
    "        if col not in categorical_cols and col != 'id' and 'timestamp' not in col:\n",
    "            categorical_cols.append(col)\n",
    "            print(f\"Added column to categorical: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle high-cardinality categorical features\n",
    "high_cardinality_threshold = 50  # Adjust this threshold as needed\n",
    "high_cardinality_cols = []\n",
    "for col in categorical_cols:\n",
    "    if train_df[col].nunique() > high_cardinality_threshold:\n",
    "        high_cardinality_cols.append(col)\n",
    "        print(f\"High cardinality column: {col} with {train_df[col].nunique()} unique values\")\n",
    "\n",
    "# For high cardinality columns, keep only the top N most frequent categories\n",
    "for col in high_cardinality_cols:\n",
    "    top_categories = train_df[col].value_counts().nlargest(high_cardinality_threshold).index\n",
    "    train_df[col] = train_df[col].apply(lambda x: x if x in top_categories else 'Other')\n",
    "    test_df[col] = test_df[col].apply(lambda x: x if x in top_categories else 'Other')\n",
    "    print(f\"Limited {col} to top {high_cardinality_threshold} categories plus 'Other'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get all remaining columns except target and id\n",
    "numerical_cols = [col for col in train_df.columns \n",
    "                 if col not in categorical_cols and col != 'target' and col != 'id' \n",
    "                 and 'timestamp' not in col]\n",
    "\n",
    "# Print column types for debugging\n",
    "print(f\"Number of numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Number of categorical columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare feature engineering pipeline\n",
    "print(\"Building preprocessing pipeline...\")\n",
    "# Numerical pipeline with imputation and scaling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline with imputation and one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ], remainder='drop', n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split features and target\n",
    "X = train_df.drop('target', axis=1)\n",
    "y = train_df['target']\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Training models...\")\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble_model = VotingRegressor([\n",
    "    ('xgb', XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('gbr', GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    )),\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Create full pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ensemble_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fit the model\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on validation set\n",
    "val_predictions = model_pipeline.predict(X_val)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prediction clipping function to enforce bounds\n",
    "def clip_predictions(preds):\n",
    "    return np.clip(preds, 1, 100)\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"Generating predictions...\")\n",
    "test_predictions = clip_predictions(model_pipeline.predict(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create submission file\n",
    "submission['target'] = test_predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance (if using XGBoost alone)\n",
    "try:\n",
    "    xgb_model = model_pipeline.named_steps['model'].estimators_[0]\n",
    "    feature_names = numerical_cols + list(model_pipeline.named_steps['preprocessor']\n",
    "                                  .named_transformers_['cat']\n",
    "                                  .named_steps['onehot']\n",
    "                                  .get_feature_names_out(categorical_cols))\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = xgb_model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False).head(20)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not extract feature importances: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
